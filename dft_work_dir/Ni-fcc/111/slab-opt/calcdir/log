  python dir          : /home/vossj/suncat/lib/python2.7/site-packages/espresso
  espresso dir        : /home/vossj/suncat/esdld/espresso-dynpy-beef/bin
  pseudo dir          : /home/alatimer/bin/psp/esp_psp_w_gbrvRu/
  ase-espresso py git : ffa2778-git


--------------------------------------------------------------------------
WARNING: It appears that your OpenFabrics subsystem is configured to only
allow registering part of your physical memory.  This can cause MPI jobs to
run with erratic performance, hang, and/or crash.

This may be caused by your OpenFabrics vendor limiting the amount of
physical memory that can be registered.  You should investigate the
relevant Linux kernel module parameters that control how much physical
memory can be registered, and increase them to allow registering all
physical memory on your machine.

See this Open MPI FAQ item for more information on these Linux kernel module
parameters:

    http://www.open-mpi.org/faq/?category=openfabrics#ib-locked-pages

  Local host:              sherlock-ln04
  Registerable memory:     16384 MiB
  Total memory:            65314 MiB

Your MPI job will continue, but may be behave poorly and/or hang.
--------------------------------------------------------------------------

     Program PWSCF v.5.1rc2 (svn rev. ) starts on  7Mar2017 at 23:42:11 

     This program is part of the open-source Quantum ESPRESSO suite
     for quantum simulation of materials; please cite
         "P. Giannozzi et al., J. Phys.:Condens. Matter 21 395502 (2009);
          URL http://www.quantum-espresso.org", 
     in publications or presentations arising from this work. More details at
     http://www.quantum-espresso.org/quote

     Parallel version (MPI), running on     1 processors
     Reading input from pw.inp
     Message from routine read_cards :
     DEPRECATED: no units specified in CELL_PARAMETERS card

     Current dimensions of program PWSCF are:
     Max number of different atomic species (ntypx) = 10
     Max number of k-points (npk) =  40000
     Max angular momentum in pseudopotentials (lmaxx) =  3
     Presently no symmetry can be used with electric field


     IMPORTANT: XC functional enforced from input :
     Exchange-correlation      = RPBE ( 1  4 28  4 0)
     Any further DFT definition will be discarded
     Please, verify this is what you really want

               file Ni.UPF: wavefunction(s)  3D renormalized

     Subspace diagonalization in iterative solution of the eigenvalue problem:
     a serial algorithm will be used


 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
     Error in routine check_atoms (1):
     atoms #   1 and #   5 differ by lattice vector ( 0,-1, 0) in crystal axis
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

     stopping ...
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD 
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
  python dir          : /home/vossj/suncat/lib/python2.7/site-packages/espresso
  espresso dir        : /home/vossj/suncat/esdld/espresso-dynpy-beef/bin
  pseudo dir          : /home/alatimer/bin/psp/esp_psp_w_gbrvRu/
  ase-espresso py git : ffa2778-git


--------------------------------------------------------------------------
WARNING: It appears that your OpenFabrics subsystem is configured to only
allow registering part of your physical memory.  This can cause MPI jobs to
run with erratic performance, hang, and/or crash.

This may be caused by your OpenFabrics vendor limiting the amount of
physical memory that can be registered.  You should investigate the
relevant Linux kernel module parameters that control how much physical
memory can be registered, and increase them to allow registering all
physical memory on your machine.

See this Open MPI FAQ item for more information on these Linux kernel module
parameters:

    http://www.open-mpi.org/faq/?category=openfabrics#ib-locked-pages

  Local host:              sherlock-ln04
  Registerable memory:     16384 MiB
  Total memory:            65314 MiB

Your MPI job will continue, but may be behave poorly and/or hang.
--------------------------------------------------------------------------

     Program PWSCF v.5.1rc2 (svn rev. ) starts on  7Mar2017 at 23:46:11 

     This program is part of the open-source Quantum ESPRESSO suite
     for quantum simulation of materials; please cite
         "P. Giannozzi et al., J. Phys.:Condens. Matter 21 395502 (2009);
          URL http://www.quantum-espresso.org", 
     in publications or presentations arising from this work. More details at
     http://www.quantum-espresso.org/quote

     Parallel version (MPI), running on     1 processors
     Reading input from pw.inp
     Message from routine read_cards :
     DEPRECATED: no units specified in CELL_PARAMETERS card

     Current dimensions of program PWSCF are:
     Max number of different atomic species (ntypx) = 10
     Max number of k-points (npk) =  40000
     Max angular momentum in pseudopotentials (lmaxx) =  3
     Presently no symmetry can be used with electric field


     IMPORTANT: XC functional enforced from input :
     Exchange-correlation      = RPBE ( 1  4 28  4 0)
     Any further DFT definition will be discarded
     Please, verify this is what you really want

               file Ni.UPF: wavefunction(s)  3D renormalized

     Subspace diagonalization in iterative solution of the eigenvalue problem:
     a serial algorithm will be used


 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
     Error in routine check_atoms (1):
     atoms #   1 and #   5 differ by lattice vector ( 0,-1, 0) in crystal axis
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

     stopping ...
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD 
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
